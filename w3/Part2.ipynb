{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Atlas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess(line):\n",
    "    line = cleanpunc(line)\n",
    "    line = cleanstop(line)\n",
    "    line = [stemming(w) for w in line]\n",
    "    return line\n",
    "    \n",
    "def cleanstop(line):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    filtered_sentence = []\n",
    "    for w in line.split():\n",
    "        if w.isalpha() and len(w)>2:\n",
    "            w = w.lower()\n",
    "            if(w not in stop):\n",
    "                filtered_sentence.append(w)\n",
    "    return filtered_sentence\n",
    "                \n",
    "def stemming(word):\n",
    "    snowball  = nltk.stem.SnowballStemmer('english')\n",
    "    return (snowball.stem(word.lower())).encode('utf8')\n",
    "\n",
    "def cleanpunc(line): \n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',line)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    return  cleaned\n",
    "\n",
    "def load_data(path):\n",
    "    with open(path,\"r\") as text_file:\n",
    "        lines = text_file.read().split('\\n')\n",
    "    lines = [line.split(\"\\t\") for line in lines if len(line.split(\"\\t\"))==2 and line.split(\"\\t\")[1]!='']\n",
    "    train_sentences = [preprocess(line[0]) for line in lines]\n",
    "    train_labels = [int(line[1]) for line in lines]\n",
    "    return pd.DataFrame({'line': train_sentences, 'label': train_labels})\n",
    "\n",
    "def word_count(data, label):\n",
    "    for line, label in zip(data.values, label.values):\n",
    "        word_counts = collections.Counter(line)\n",
    "        for word, count in sorted(word_counts.items()):\n",
    "            if word not in WORD_COUNTS.keys():\n",
    "                WORD_COUNTS[word] = count\n",
    "            else:\n",
    "                WORD_COUNTS[word] += count\n",
    "            if word not in POS_NEG_WORDS[label].keys():\n",
    "                POS_NEG_WORDS[label][word] = count\n",
    "            else:\n",
    "                POS_NEG_WORDS[label][word] += count\n",
    "\n",
    "\n",
    "def prior(y_data, classes):\n",
    "    priors = np.zeros(len(classes))\n",
    "    for i, c in enumerate(classes):\n",
    "        priors[i] = len(y_data[y_data==c]) / len(y_data)\n",
    "    return priors\n",
    "def p_x_y(X,y):\n",
    "    result = []\n",
    "    label_total = 0\n",
    "    for w in POS_NEG_WORDS[y].keys():\n",
    "        label_total += POS_NEG_WORDS[y][w]\n",
    "    for j, line in enumerate(X.values):\n",
    "        result.append(_p_x_Y(line, y, label_total))\n",
    "    return np.array(result)   \n",
    "\n",
    "def _p_x_Y(line, label, label_total):\n",
    "    prob = 0\n",
    "    for w in line:\n",
    "        p = 0\n",
    "        if w in POS_NEG_WORDS[label].keys():\n",
    "            p = (POS_NEG_WORDS[label][w] / label_total)+0.1\n",
    "        else:\n",
    "            p = 0.1\n",
    "        prob += np.log(p)\n",
    "    return prob\n",
    "        \n",
    "def fit(X_data, y_data):\n",
    "    classes = np.unique(y_data)\n",
    "    classes.sort()\n",
    "#     priors = prior(classes)\n",
    "    priors = prior(y_data, classes)\n",
    "    result = np.zeros((X_data.shape[0], len(classes)))\n",
    "    for i, y in enumerate(classes):\n",
    "        X = X_data#[y_data == y]\n",
    "        pxy_py = p_x_y(X, y) + priors[i]\n",
    "        result[:,i] =  pxy_py\n",
    "    return np.argmax(result, axis=1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon Dataset\n",
      "Test Accuracy: \n",
      "Total:  0.705\n",
      "Class 0 :  0.49\n",
      "Class 1 :  0.92\n",
      "Train Accuracy: \n",
      "Total:  0.815\n",
      "Class 0 :  0.6725\n",
      "Class 1 :  0.9575\n"
     ]
    }
   ],
   "source": [
    "POS_NEG_WORDS = {0: dict(), 1: dict()}\n",
    "WORD_COUNTS = {}\n",
    "data = load_data('amazon_cells_labelled.txt')\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['line'],data['label'], test_size=0.2, stratify=data['label'] ,random_state=42)\n",
    "word_count(X_train, y_train)\n",
    "fit(X_test, y_test)\n",
    "# WORD_COUNTS = pd.DataFrame.from_dict(word_count(data), orient='index',columns=['count'])\n",
    "print('Amazon Dataset')\n",
    "print(\"Test Accuracy: \")\n",
    "predicted = fit(X_test, y_test)\n",
    "print('Total: ', accuracy_score(y_test, predicted))\n",
    "y_test_0 = y_test[y_test.isin([0])]\n",
    "y_test_1 = y_test[y_test.isin([1])]\n",
    "predicted_0 = predicted[y_test.isin([0])]\n",
    "predicted_1 = predicted[y_test.isin([1])]\n",
    "print(\"Class 0 : \", accuracy_score(y_test_0, predicted_0))\n",
    "print(\"Class 1 : \", accuracy_score(y_test_1, predicted_1))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Train Accuracy: \")\n",
    "predicted = fit(X_train, y_train)\n",
    "print('Total: ', accuracy_score(y_train, predicted))\n",
    "y_train_0 = y_train[y_train.isin([0])]\n",
    "y_train_1 = y_train[y_train.isin([1])]\n",
    "predicted_0 = predicted[y_train.isin([0])]\n",
    "predicted_1 = predicted[y_train.isin([1])]\n",
    "print(\"Class 0 : \", accuracy_score(y_train_0, predicted_0))\n",
    "print(\"Class 1 : \", accuracy_score(y_train_1, predicted_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB Dataset\n",
      "Test Accuracy: \n",
      "Total:  0.715\n",
      "Class 0 :  0.73\n",
      "Class 1 :  0.7\n",
      "Train Accuracy: \n",
      "Total:  0.82375\n",
      "Class 0 :  0.8575\n",
      "Class 1 :  0.79\n"
     ]
    }
   ],
   "source": [
    "POS_NEG_WORDS = {0: dict(), 1: dict()}\n",
    "WORD_COUNTS = {}\n",
    "data = load_data('imdb_labelled.txt')\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['line'],data['label'], test_size=0.2, stratify=data['label'] ,random_state=42)\n",
    "word_count(X_train, y_train)\n",
    "print('IMDB Dataset')\n",
    "print(\"Test Accuracy: \")\n",
    "predicted = fit(X_test, y_test)\n",
    "print('Total: ', accuracy_score(y_test, predicted))\n",
    "y_test_0 = y_test[y_test.isin([0])]\n",
    "y_test_1 = y_test[y_test.isin([1])]\n",
    "predicted_0 = predicted[y_test.isin([0])]\n",
    "predicted_1 = predicted[y_test.isin([1])]\n",
    "print(\"Class 0 : \", accuracy_score(y_test_0, predicted_0))\n",
    "print(\"Class 1 : \", accuracy_score(y_test_1, predicted_1))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Train Accuracy: \")\n",
    "predicted = fit(X_train, y_train)\n",
    "print('Total: ', accuracy_score(y_train, predicted))\n",
    "y_train_0 = y_train[y_train.isin([0])]\n",
    "y_train_1 = y_train[y_train.isin([1])]\n",
    "predicted_0 = predicted[y_train.isin([0])]\n",
    "predicted_1 = predicted[y_train.isin([1])]\n",
    "print(\"Class 0 : \", accuracy_score(y_train_0, predicted_0))\n",
    "print(\"Class 1 : \", accuracy_score(y_train_1, predicted_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yelp Dataset\n",
      "Test Accuracy: \n",
      "Total:  0.755\n",
      "Class 0 :  0.67\n",
      "Class 1 :  0.84\n",
      "Train Accuracy: \n",
      "Total:  0.85625\n",
      "Class 0 :  0.835\n",
      "Class 1 :  0.8775\n"
     ]
    }
   ],
   "source": [
    "POS_NEG_WORDS = {0: dict(), 1: dict()}\n",
    "WORD_COUNTS = {}\n",
    "data = load_data('yelp_labelled.txt')\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['line'],data['label'], test_size=0.2, stratify=data['label'] ,random_state=42)\n",
    "word_count(X_train, y_train)\n",
    "print(\"Yelp Dataset\")\n",
    "print(\"Test Accuracy: \")\n",
    "predicted = fit(X_test, y_test)\n",
    "print('Total: ', accuracy_score(y_test, predicted))\n",
    "y_test_0 = y_test[y_test.isin([0])]\n",
    "y_test_1 = y_test[y_test.isin([1])]\n",
    "predicted_0 = predicted[y_test.isin([0])]\n",
    "predicted_1 = predicted[y_test.isin([1])]\n",
    "print(\"Class 0 : \", accuracy_score(y_test_0, predicted_0))\n",
    "print(\"Class 1 : \", accuracy_score(y_test_1, predicted_1))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Train Accuracy: \")\n",
    "predicted = fit(X_train, y_train)\n",
    "print('Total: ', accuracy_score(y_train, predicted))\n",
    "y_train_0 = y_train[y_train.isin([0])]\n",
    "y_train_1 = y_train[y_train.isin([1])]\n",
    "predicted_0 = predicted[y_train.isin([0])]\n",
    "predicted_1 = predicted[y_train.isin([1])]\n",
    "print(\"Class 0 : \", accuracy_score(y_train_0, predicted_0))\n",
    "print(\"Class 1 : \", accuracy_score(y_train_1, predicted_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
